{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6129a142",
   "metadata": {},
   "source": [
    "# Artificial Neural Network and Deep Learning\n",
    "- Artificial Neural Networks (ANNs) and Deep Learning are closely related concepts, with Deep Learning being a subset of machine learning that specifically involves the use of deep neural networks\n",
    "  - ANNs are computational models inspired by the structure and functioning of the human brain. They consist of interconnected nodes organized into layers: an input layer, one or more hidden layers, and an output layer.\n",
    "  - Deep Learning is a subfield of machine learning that focuses on neural networks with multiple hidden layers, often referred to as deep neural networks. The term \"deep\" refers to the depth of the network, i.e., the number of layers it has."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a432616",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63b906fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn datasets are included as part of the scikit-learn (sklearn) library, so they come pre-installed with the library. \n",
    "# Due to this, we can easily access and load these datasets, without having to download them separately.\n",
    "\n",
    "# Here, we load the Iris dataset\n",
    "# This is a popular dataset often used for practicing and demonstrating machine learning and classification algorithms\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# imports the numpy and pandas libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d743a08d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
      "       [4.9, 3. , 1.4, 0.2],\n",
      "       [4.7, 3.2, 1.3, 0.2],\n",
      "       [4.6, 3.1, 1.5, 0.2],\n",
      "       [5. , 3.6, 1.4, 0.2],\n",
      "       [5.4, 3.9, 1.7, 0.4],\n",
      "       [4.6, 3.4, 1.4, 0.3],\n",
      "       [5. , 3.4, 1.5, 0.2],\n",
      "       [4.4, 2.9, 1.4, 0.2],\n",
      "       [4.9, 3.1, 1.5, 0.1],\n",
      "       [5.4, 3.7, 1.5, 0.2],\n",
      "       [4.8, 3.4, 1.6, 0.2],\n",
      "       [4.8, 3. , 1.4, 0.1],\n",
      "       [4.3, 3. , 1.1, 0.1],\n",
      "       [5.8, 4. , 1.2, 0.2],\n",
      "       [5.7, 4.4, 1.5, 0.4],\n",
      "       [5.4, 3.9, 1.3, 0.4],\n",
      "       [5.1, 3.5, 1.4, 0.3],\n",
      "       [5.7, 3.8, 1.7, 0.3],\n",
      "       [5.1, 3.8, 1.5, 0.3],\n",
      "       [5.4, 3.4, 1.7, 0.2],\n",
      "       [5.1, 3.7, 1.5, 0.4],\n",
      "       [4.6, 3.6, 1. , 0.2],\n",
      "       [5.1, 3.3, 1.7, 0.5],\n",
      "       [4.8, 3.4, 1.9, 0.2],\n",
      "       [5. , 3. , 1.6, 0.2],\n",
      "       [5. , 3.4, 1.6, 0.4],\n",
      "       [5.2, 3.5, 1.5, 0.2],\n",
      "       [5.2, 3.4, 1.4, 0.2],\n",
      "       [4.7, 3.2, 1.6, 0.2],\n",
      "       [4.8, 3.1, 1.6, 0.2],\n",
      "       [5.4, 3.4, 1.5, 0.4],\n",
      "       [5.2, 4.1, 1.5, 0.1],\n",
      "       [5.5, 4.2, 1.4, 0.2],\n",
      "       [4.9, 3.1, 1.5, 0.2],\n",
      "       [5. , 3.2, 1.2, 0.2],\n",
      "       [5.5, 3.5, 1.3, 0.2],\n",
      "       [4.9, 3.6, 1.4, 0.1],\n",
      "       [4.4, 3. , 1.3, 0.2],\n",
      "       [5.1, 3.4, 1.5, 0.2],\n",
      "       [5. , 3.5, 1.3, 0.3],\n",
      "       [4.5, 2.3, 1.3, 0.3],\n",
      "       [4.4, 3.2, 1.3, 0.2],\n",
      "       [5. , 3.5, 1.6, 0.6],\n",
      "       [5.1, 3.8, 1.9, 0.4],\n",
      "       [4.8, 3. , 1.4, 0.3],\n",
      "       [5.1, 3.8, 1.6, 0.2],\n",
      "       [4.6, 3.2, 1.4, 0.2],\n",
      "       [5.3, 3.7, 1.5, 0.2],\n",
      "       [5. , 3.3, 1.4, 0.2],\n",
      "       [7. , 3.2, 4.7, 1.4],\n",
      "       [6.4, 3.2, 4.5, 1.5],\n",
      "       [6.9, 3.1, 4.9, 1.5],\n",
      "       [5.5, 2.3, 4. , 1.3],\n",
      "       [6.5, 2.8, 4.6, 1.5],\n",
      "       [5.7, 2.8, 4.5, 1.3],\n",
      "       [6.3, 3.3, 4.7, 1.6],\n",
      "       [4.9, 2.4, 3.3, 1. ],\n",
      "       [6.6, 2.9, 4.6, 1.3],\n",
      "       [5.2, 2.7, 3.9, 1.4],\n",
      "       [5. , 2. , 3.5, 1. ],\n",
      "       [5.9, 3. , 4.2, 1.5],\n",
      "       [6. , 2.2, 4. , 1. ],\n",
      "       [6.1, 2.9, 4.7, 1.4],\n",
      "       [5.6, 2.9, 3.6, 1.3],\n",
      "       [6.7, 3.1, 4.4, 1.4],\n",
      "       [5.6, 3. , 4.5, 1.5],\n",
      "       [5.8, 2.7, 4.1, 1. ],\n",
      "       [6.2, 2.2, 4.5, 1.5],\n",
      "       [5.6, 2.5, 3.9, 1.1],\n",
      "       [5.9, 3.2, 4.8, 1.8],\n",
      "       [6.1, 2.8, 4. , 1.3],\n",
      "       [6.3, 2.5, 4.9, 1.5],\n",
      "       [6.1, 2.8, 4.7, 1.2],\n",
      "       [6.4, 2.9, 4.3, 1.3],\n",
      "       [6.6, 3. , 4.4, 1.4],\n",
      "       [6.8, 2.8, 4.8, 1.4],\n",
      "       [6.7, 3. , 5. , 1.7],\n",
      "       [6. , 2.9, 4.5, 1.5],\n",
      "       [5.7, 2.6, 3.5, 1. ],\n",
      "       [5.5, 2.4, 3.8, 1.1],\n",
      "       [5.5, 2.4, 3.7, 1. ],\n",
      "       [5.8, 2.7, 3.9, 1.2],\n",
      "       [6. , 2.7, 5.1, 1.6],\n",
      "       [5.4, 3. , 4.5, 1.5],\n",
      "       [6. , 3.4, 4.5, 1.6],\n",
      "       [6.7, 3.1, 4.7, 1.5],\n",
      "       [6.3, 2.3, 4.4, 1.3],\n",
      "       [5.6, 3. , 4.1, 1.3],\n",
      "       [5.5, 2.5, 4. , 1.3],\n",
      "       [5.5, 2.6, 4.4, 1.2],\n",
      "       [6.1, 3. , 4.6, 1.4],\n",
      "       [5.8, 2.6, 4. , 1.2],\n",
      "       [5. , 2.3, 3.3, 1. ],\n",
      "       [5.6, 2.7, 4.2, 1.3],\n",
      "       [5.7, 3. , 4.2, 1.2],\n",
      "       [5.7, 2.9, 4.2, 1.3],\n",
      "       [6.2, 2.9, 4.3, 1.3],\n",
      "       [5.1, 2.5, 3. , 1.1],\n",
      "       [5.7, 2.8, 4.1, 1.3],\n",
      "       [6.3, 3.3, 6. , 2.5],\n",
      "       [5.8, 2.7, 5.1, 1.9],\n",
      "       [7.1, 3. , 5.9, 2.1],\n",
      "       [6.3, 2.9, 5.6, 1.8],\n",
      "       [6.5, 3. , 5.8, 2.2],\n",
      "       [7.6, 3. , 6.6, 2.1],\n",
      "       [4.9, 2.5, 4.5, 1.7],\n",
      "       [7.3, 2.9, 6.3, 1.8],\n",
      "       [6.7, 2.5, 5.8, 1.8],\n",
      "       [7.2, 3.6, 6.1, 2.5],\n",
      "       [6.5, 3.2, 5.1, 2. ],\n",
      "       [6.4, 2.7, 5.3, 1.9],\n",
      "       [6.8, 3. , 5.5, 2.1],\n",
      "       [5.7, 2.5, 5. , 2. ],\n",
      "       [5.8, 2.8, 5.1, 2.4],\n",
      "       [6.4, 3.2, 5.3, 2.3],\n",
      "       [6.5, 3. , 5.5, 1.8],\n",
      "       [7.7, 3.8, 6.7, 2.2],\n",
      "       [7.7, 2.6, 6.9, 2.3],\n",
      "       [6. , 2.2, 5. , 1.5],\n",
      "       [6.9, 3.2, 5.7, 2.3],\n",
      "       [5.6, 2.8, 4.9, 2. ],\n",
      "       [7.7, 2.8, 6.7, 2. ],\n",
      "       [6.3, 2.7, 4.9, 1.8],\n",
      "       [6.7, 3.3, 5.7, 2.1],\n",
      "       [7.2, 3.2, 6. , 1.8],\n",
      "       [6.2, 2.8, 4.8, 1.8],\n",
      "       [6.1, 3. , 4.9, 1.8],\n",
      "       [6.4, 2.8, 5.6, 2.1],\n",
      "       [7.2, 3. , 5.8, 1.6],\n",
      "       [7.4, 2.8, 6.1, 1.9],\n",
      "       [7.9, 3.8, 6.4, 2. ],\n",
      "       [6.4, 2.8, 5.6, 2.2],\n",
      "       [6.3, 2.8, 5.1, 1.5],\n",
      "       [6.1, 2.6, 5.6, 1.4],\n",
      "       [7.7, 3. , 6.1, 2.3],\n",
      "       [6.3, 3.4, 5.6, 2.4],\n",
      "       [6.4, 3.1, 5.5, 1.8],\n",
      "       [6. , 3. , 4.8, 1.8],\n",
      "       [6.9, 3.1, 5.4, 2.1],\n",
      "       [6.7, 3.1, 5.6, 2.4],\n",
      "       [6.9, 3.1, 5.1, 2.3],\n",
      "       [5.8, 2.7, 5.1, 1.9],\n",
      "       [6.8, 3.2, 5.9, 2.3],\n",
      "       [6.7, 3.3, 5.7, 2.5],\n",
      "       [6.7, 3. , 5.2, 2.3],\n",
      "       [6.3, 2.5, 5. , 1.9],\n",
      "       [6.5, 3. , 5.2, 2. ],\n",
      "       [6.2, 3.4, 5.4, 2.3],\n",
      "       [5.9, 3. , 5.1, 1.8]]), 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), 'frame': None, 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'), 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...', 'feature_names': ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'], 'filename': 'iris.csv', 'data_module': 'sklearn.datasets.data'}\n"
     ]
    }
   ],
   "source": [
    "# The load_iris function is called to load the Iris dataset\n",
    "iris = load_iris()\n",
    "print(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bb94bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                  5.1               3.5                1.4               0.2   \n",
       "1                  4.9               3.0                1.4               0.2   \n",
       "2                  4.7               3.2                1.3               0.2   \n",
       "3                  4.6               3.1                1.5               0.2   \n",
       "4                  5.0               3.6                1.4               0.2   \n",
       "..                 ...               ...                ...               ...   \n",
       "145                6.7               3.0                5.2               2.3   \n",
       "146                6.3               2.5                5.0               1.9   \n",
       "147                6.5               3.0                5.2               2.0   \n",
       "148                6.2               3.4                5.4               2.3   \n",
       "149                5.9               3.0                5.1               1.8   \n",
       "\n",
       "     target  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "..      ...  \n",
       "145       2  \n",
       "146       2  \n",
       "147       2  \n",
       "148       2  \n",
       "149       2  \n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creates a Pandas DataFrame named iris_df using the Iris dataset's feature_names \n",
    "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "\n",
    "# A new column named \"target\" is added to the iris_df DataFrame. The values for this column are taken from iris.target\n",
    "iris_df[\"target\"] = iris.target\n",
    "\n",
    "# Display the dataframe\n",
    "iris_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "775ea409",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine the shape or dimensions of the Pandas DataFrame iris_df\n",
    "iris_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdb06edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)',\n",
       "       'petal width (cm)', 'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve and display the column names of the Pandas DataFrame iris_df\n",
    "iris_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcfa8faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   sepal length (cm)  150 non-null    float64\n",
      " 1   sepal width (cm)   150 non-null    float64\n",
      " 2   petal length (cm)  150 non-null    float64\n",
      " 3   petal width (cm)   150 non-null    float64\n",
      " 4   target             150 non-null    int32  \n",
      "dtypes: float64(4), int32(1)\n",
      "memory usage: 5.4 KB\n"
     ]
    }
   ],
   "source": [
    "# Obtain a concise summary of the information about the Pandas DataFrame iris_df\n",
    "iris_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6b99a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.057333</td>\n",
       "      <td>3.758000</td>\n",
       "      <td>1.199333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.435866</td>\n",
       "      <td>1.765298</td>\n",
       "      <td>0.762238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
       "count         150.000000        150.000000         150.000000   \n",
       "mean            5.843333          3.057333           3.758000   \n",
       "std             0.828066          0.435866           1.765298   \n",
       "min             4.300000          2.000000           1.000000   \n",
       "25%             5.100000          2.800000           1.600000   \n",
       "50%             5.800000          3.000000           4.350000   \n",
       "75%             6.400000          3.300000           5.100000   \n",
       "max             7.900000          4.400000           6.900000   \n",
       "\n",
       "       petal width (cm)  \n",
       "count        150.000000  \n",
       "mean           1.199333  \n",
       "std            0.762238  \n",
       "min            0.100000  \n",
       "25%            0.300000  \n",
       "50%            1.300000  \n",
       "75%            1.800000  \n",
       "max            2.500000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain a statistical summary of the numerical columns in a Pandas DataFrame iris_df while excluding the last (target) column\n",
    "iris_df.iloc[:,:-1].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7655eff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sepal length (cm)    0\n",
       "sepal width (cm)     0\n",
       "petal length (cm)    0\n",
       "petal width (cm)     0\n",
       "target               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of missing (null) values in each column of the Pandas DataFrame iris_df\n",
    "iris_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c8731c",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ad65f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the train_test_split function from the sklearn.model_selection module \n",
    "# The train_test_split function is used to split a dataset into training and testing subsets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9061c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                  5.1               3.5                1.4               0.2\n",
       "1                  4.9               3.0                1.4               0.2\n",
       "2                  4.7               3.2                1.3               0.2\n",
       "3                  4.6               3.1                1.5               0.2\n",
       "4                  5.0               3.6                1.4               0.2\n",
       "..                 ...               ...                ...               ...\n",
       "145                6.7               3.0                5.2               2.3\n",
       "146                6.3               2.5                5.0               1.9\n",
       "147                6.5               3.0                5.2               2.0\n",
       "148                6.2               3.4                5.4               2.3\n",
       "149                5.9               3.0                5.1               1.8\n",
       "\n",
       "[150 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new variable X is created to store the feature variables for your machine learning model\n",
    "X = iris_df.drop([\"target\"], axis=1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f542e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "145    2\n",
       "146    2\n",
       "147    2\n",
       "148    2\n",
       "149    2\n",
       "Name: target, Length: 150, dtype: int32"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A new variable y is created to store the target variable (class labels) for your machine learning model\n",
    "y = iris_df.iloc[:,-1]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "776ecd5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, array([0, 1, 2]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n_features: This variable is assigned the number of features in our dataset. \n",
    "# n_classes: This variable is assigned an array containing the unique classes or labels found in our target variable y\n",
    "n_features, n_classes = X.shape[1], np.unique(y)\n",
    "\n",
    "n_features, n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cf49e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4) (120,) (30, 4) (30,)\n"
     ]
    }
   ],
   "source": [
    "# train_test_split is used to split features X and target variable y into training and testing sets\n",
    "# test_size parameter specifies the proportion of the data allocated for the testing set, which is set to 20% in this case.\n",
    "# X_train: This variable contains the feature data for the training set.\n",
    "# X_test: This variable contains the feature data for the testing set.\n",
    "# y_train: This variable contains the target labels for the training set.\n",
    "# y_test: This variable contains the target labels for the testing set.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Print the shapes of the training and testing sets\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daa2a48",
   "metadata": {},
   "source": [
    "### Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f9b800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imported MLPClassifier from sklearn.neural_network for building the artificial neural network\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d50299e",
   "metadata": {},
   "source": [
    "#### Single Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51ae3499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the MLPClassifier with specified parameters\n",
    "# learning_rate_init=0.1 means that the initial learning rate for the optimizer is set to 0.1. \n",
    "#   A higher learning rate might result in faster convergence but could risk overshooting the optimal values\n",
    "#   A lower learning rate might converge more slowly but with more stability\n",
    "# max_iter=1000 means that the training process will continue for a maximum of 1000 iterations\n",
    "# Note: The learning_rate_init, max_iter, and random_state are parameters you can experiment with\n",
    "ann_single = MLPClassifier(learning_rate_init=0.1, \n",
    "                           max_iter=1000, \n",
    "                           random_state=42)\n",
    "\n",
    "# Train the neural network on the training data\n",
    "ann_single.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the performance on the training and testing sets\n",
    "test_score = ann_single.score(X_test, y_test)\n",
    "\n",
    "# Print the accuracy scores\n",
    "print(\"Test accuracy:\", test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db56696",
   "metadata": {},
   "source": [
    "#### Multiple Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d65212d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the MLPClassifier with specified parameters\n",
    "# hidden_layer_sizes=(6, 3) means that there are two hidden layers in the neural network\n",
    "#  The first hidden layer has 6 neurons, and the second hidden layer has 3 neurons\n",
    "# Note: The learning_rate_init, max_iter, random_state, and hidden_layer_sizes are parameters you can experiment with\n",
    "#  You might start experimenting with a single hidden layer and gradually increase the complexity by adding more layers\n",
    "#  Neurons in a Layer: You might try to start with a small number and then increase it gradually\n",
    "#    A common practice is to use a number between the input and output layer sizes\n",
    "#    Be cautious about overfitting, especially if you have limited data. Too many neurons or layers might lead to overfitting\n",
    "ann_multi = MLPClassifier(learning_rate_init=0.01, \n",
    "                          max_iter=1000, \n",
    "                          random_state=42,\n",
    "                          hidden_layer_sizes=(6, 3))\n",
    "\n",
    "# Train the neural network on the training data\n",
    "ann_multi.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the performance on the training and testing sets\n",
    "test_score = ann_multi.score(X_test, y_test)\n",
    "\n",
    "# Print the accuracy scores\n",
    "print(\"Test accuracy:\", test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c325ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weights between the input and the first hidden layer:\n",
      "[[ 1.35548786e-01  9.35074270e-01  2.90140874e-01  2.36230193e-01\n",
      "   1.49513984e-01  7.87937941e-07]\n",
      " [-4.48276098e-01  8.32861958e-01  1.96166268e-01  4.34029746e-01\n",
      "   2.13066288e-01 -2.80345931e-05]\n",
      " [ 1.10135975e+00 -5.85440448e-01 -5.31808850e-01 -5.13461958e-01\n",
      "  -3.83708475e-01  3.64758452e-08]\n",
      " [ 7.80510031e-01 -8.73392646e-01  2.17877653e-01 -7.86113275e-01\n",
      "  -9.10019110e-01  2.37384604e-07]]\n",
      "The weights between the first and the second hidden layer:\n",
      "[[ 1.43025887e+00 -9.58400615e-01 -1.19622868e+00]\n",
      " [ 6.64030899e-01  1.32967910e+00  6.99983063e-01]\n",
      " [-6.59034752e-01 -4.56715847e-01  6.93084076e-01]\n",
      " [-4.63672159e-01 -4.38086046e-01  9.48497909e-01]\n",
      " [-1.72853441e+00  1.91990238e+00  6.63990611e-01]\n",
      " [ 3.58820238e-07  2.15898832e-07  2.87118704e-08]]\n",
      "The weights between the second hidden layer and the output:\n",
      "[[ 0.16593569  0.83495449  1.29936906]\n",
      " [ 0.77072384  0.99310783 -1.79600196]\n",
      " [-0.08678473 -1.34581221 -0.58831602]]\n"
     ]
    }
   ],
   "source": [
    "print(\"The weights between the input and the first hidden layer:\")\n",
    "print(ann_multi.coefs_[0])\n",
    "print(\"The weights between the first and the second hidden layer:\")\n",
    "print(ann_multi.coefs_[1])\n",
    "print(\"The weights between the second hidden layer and the output:\")\n",
    "print(ann_multi.coefs_[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c6c1030",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0 =  0.13554878607253265\n",
      "W1 =  -0.4482760978066613\n",
      "W2 =  1.1013597471410799\n",
      "W3 =  0.7805100313380386\n"
     ]
    }
   ],
   "source": [
    "# The weights a layer with a specific neuron \n",
    "# E.g., between the input layer with the first neural of the first hidden layer\n",
    "print(\"W0 = \", ann_multi.coefs_[0][0][0])\n",
    "print(\"W1 = \", ann_multi.coefs_[0][1][0])\n",
    "print(\"W2 = \", ann_multi.coefs_[0][2][0])\n",
    "print(\"W3 = \", ann_multi.coefs_[0][3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a023feed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias for the first hidden layers:\n",
      "[-0.44711639  1.24593199 -0.58109641  0.11546416  2.39859523 -0.702636  ]\n",
      "Bias for the second hidden layers:\n",
      "[0.3205625  0.07275098 0.56073727]\n"
     ]
    }
   ],
   "source": [
    "# Bias\n",
    "# In simple words, neural network bias can be defined as the constant which is added to the product of features and weights\n",
    "#   It is used to offset the result\n",
    "#   It helps the models to shift the activation function towards the positive or negative side\n",
    "print(\"Bias for the first hidden layers:\")\n",
    "print(ann_multi.intercepts_[0])\n",
    "print(\"Bias for the second hidden layers:\")\n",
    "print(ann_multi.intercepts_[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f333fc",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2289fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17326148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  0  0]\n",
      " [ 0 10  0]\n",
      " [ 0  0 10]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the confusion matrix based on the actual target labels (y_test) and the predicted labels (ann_multi_pred)\n",
    "ann_multi_pred = ann_multi.predict(X_test)\n",
    "cm = confusion_matrix(y_test, ann_multi_pred)\n",
    "\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4534e7d",
   "metadata": {},
   "source": [
    "### Generate seaborn heatmap for the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2046c934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3fad646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creates a Pandas DataFrame df_cm to store the values of the confusion matrix\n",
    "# data: This is the confusion matrix data calculated in the previous step\n",
    "# columns=np.unique(y_test): This sets the column labels of the DataFrame\n",
    "# index=np.unique(y_test): This sets the row labels of the DataFrame\n",
    "df_cm = pd.DataFrame(cm, columns=np.unique(y_test), index = np.unique(y_test))\n",
    "\n",
    "# Set the names for the DataFrame's index (rows) and columns\n",
    "df_cm.index.name = 'Actual'\n",
    "df_cm.columns.name = 'Predicted'\n",
    "\n",
    "# Create a new figure for the heatmap \n",
    "plt.figure(figsize = (4,4))\n",
    "\n",
    "# Generate the heatmap visualization\n",
    "# df_cm: This is the Pandas DataFrame containing the confusion matrix data\n",
    "# annot=True: This parameter specifies that  cell values in the heatmap should be annotated (displayed) on the heatmap cells\n",
    "# annot_kws={\"size\": 12}: This parameter sets the font size for the annotations to 12\n",
    "# cbar=False: This parameter indicates that a color bar (legend) should not be displayed on the heatmap.\n",
    "# square=True: This parameter ensures that the heatmap cells are square\n",
    "# fmt=\"d\": This parameter specifies that the cell values should be formatted as integers\n",
    "# cmap=\"Reds\": This parameter sets the color map to \"Reds\".\n",
    "sb.heatmap(df_cm, annot=True, annot_kws={\"size\": 12}, cbar=False, square=True, fmt=\"d\", cmap=\"Reds\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57662257",
   "metadata": {},
   "source": [
    "### Deep Neural Network with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4820eb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Anonymous\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22a1e326",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# Keras requires your output feature to be one-hot encoded values\n",
    "y_train_one_hot = to_categorical(y_train, num_classes=3)\n",
    "y_test_one_hot = to_categorical(y_test, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f92527eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Anonymous\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Specify to keras that we are creating model sequentially and the output of each layer we add is input to the next layer\n",
    "dnn = Sequential()\n",
    "\n",
    "# The dense layer is a neural network layer that is connected deeply\n",
    "#   which means each neuron in the dense layer receives input from all neurons of its previous layer\n",
    "# dnn.add is used to add a layer to our neural network\n",
    "\n",
    "# Create the first hidden layer of the neural network.\n",
    "#   The layer has 16 neurons \n",
    "#   input_shape=(X.shape[1], ) indicates that the input to this layer is the number of features in the input data\n",
    "#   activation='relu' specifies the Rectified Linear Unit (ReLU) activation function\n",
    "dnn.add(Dense(16, input_shape=(X.shape[1],), activation='relu'))\n",
    "        \n",
    "# This adds the second hidden layer to the neural network\n",
    "#   It has 8 neurons\n",
    "#   There is no need to specify the input shape in this layer because Keras can automatically infer it from the previous layer\n",
    "#   Again, the ReLU activation function is used\n",
    "dnn.add(Dense(8, activation='relu'))\n",
    "     \n",
    "# This adds the output layer to the neural network\n",
    "#   It has 3 neurons, which is suitable for a classification task with three classes\n",
    "#   The activation function used here is 'softmax', which is often used when we have more than two classes \n",
    "#     If you have two classes, you might use 'sigmoid' instead, making it suitable for binary classification tasks\n",
    "dnn.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ba53f3d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 16)                80        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 27        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 243 (972.00 Byte)\n",
      "Trainable params: 243 (972.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Looking at the summary of the model\n",
    "dnn.summary()\n",
    "\n",
    "# Param #: This column shows the number of parameters (weights and biases) in each layer \n",
    "# These parameters are the variables that the neural network learns during training to make predictions.\n",
    "#   For a Dense layer, the number of parameters is calculated as input_dim * output_dim + output_dim\n",
    "#     Here, input_dim is the number of input features to the layer, and output_dim is the number of neurons in the layer.\n",
    "#     For example, in a Dense layer with 10 input features and 5 output neurons, \n",
    "#       the number of parameters would be (10 * 5) + 5 = 55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22a62508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Anonymous\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Configuring the learning process of a neural network model\n",
    "# loss='categorical_crossentropy': Suitable when your target variable is one-hot encoded\n",
    "#    for example: Class A: [1, 0, 0], Class B: [0, 1, 0], Class C: [0, 0, 1]\n",
    "#      sparse_categorical_crossentropy: If your target variable is not one-hot encoded (i.e., it's represented as integers), \n",
    "#      binary_crossentropy: Commonly used for binary classification problems\n",
    "# optimizer='adam': This parameter specifies the optimization algorithm to be used during training. \n",
    "#   Adam is a popular optimization algorithm that adapts the learning rates of each parameter during training\n",
    "# metrics=['accuracy']: This parameter specifies the evaluation metric(s) to be used to monitor the performance\n",
    "dnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6069f775",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "WARNING:tensorflow:From C:\\Users\\Anonymous\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Anonymous\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "12/12 [==============================] - 2s 3ms/step - loss: 1.2277 - accuracy: 0.3333\n",
      "Epoch 2/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.1385 - accuracy: 0.3333\n",
      "Epoch 3/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.0840 - accuracy: 0.3333\n",
      "Epoch 4/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.0558 - accuracy: 0.3333\n",
      "Epoch 5/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.0439 - accuracy: 0.3333\n",
      "Epoch 6/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.0355 - accuracy: 0.3333\n",
      "Epoch 7/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.0258 - accuracy: 0.3333\n",
      "Epoch 8/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.0159 - accuracy: 0.3333\n",
      "Epoch 9/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.0049 - accuracy: 0.3333\n",
      "Epoch 10/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9937 - accuracy: 0.3333\n",
      "Epoch 11/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9815 - accuracy: 0.3333\n",
      "Epoch 12/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9681 - accuracy: 0.3750\n",
      "Epoch 13/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9547 - accuracy: 0.4833\n",
      "Epoch 14/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9408 - accuracy: 0.5750\n",
      "Epoch 15/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9255 - accuracy: 0.6167\n",
      "Epoch 16/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.9110 - accuracy: 0.6333\n",
      "Epoch 17/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8975 - accuracy: 0.6500\n",
      "Epoch 18/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8851 - accuracy: 0.6500\n",
      "Epoch 19/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8731 - accuracy: 0.6500\n",
      "Epoch 20/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8622 - accuracy: 0.6500\n",
      "Epoch 21/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8510 - accuracy: 0.6500\n",
      "Epoch 22/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8412 - accuracy: 0.6500\n",
      "Epoch 23/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8312 - accuracy: 0.6500\n",
      "Epoch 24/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8068 - accuracy: 0.6500\n",
      "Epoch 25/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7799 - accuracy: 0.6583\n",
      "Epoch 26/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7662 - accuracy: 0.6583\n",
      "Epoch 27/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7560 - accuracy: 0.6667\n",
      "Epoch 28/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7466 - accuracy: 0.6667\n",
      "Epoch 29/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7393 - accuracy: 0.6667\n",
      "Epoch 30/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7335 - accuracy: 0.6583\n",
      "Epoch 31/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7284 - accuracy: 0.6667\n",
      "Epoch 32/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7234 - accuracy: 0.6667\n",
      "Epoch 33/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7185 - accuracy: 0.6667\n",
      "Epoch 34/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7142 - accuracy: 0.6667\n",
      "Epoch 35/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7090 - accuracy: 0.6667\n",
      "Epoch 36/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7057 - accuracy: 0.6667\n",
      "Epoch 37/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.7003 - accuracy: 0.6667\n",
      "Epoch 38/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6967 - accuracy: 0.6667\n",
      "Epoch 39/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6914 - accuracy: 0.6667\n",
      "Epoch 40/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6873 - accuracy: 0.6667\n",
      "Epoch 41/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6837 - accuracy: 0.6667\n",
      "Epoch 42/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6790 - accuracy: 0.6667\n",
      "Epoch 43/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6749 - accuracy: 0.6667\n",
      "Epoch 44/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6714 - accuracy: 0.6750\n",
      "Epoch 45/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6673 - accuracy: 0.6750\n",
      "Epoch 46/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6632 - accuracy: 0.6750\n",
      "Epoch 47/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6593 - accuracy: 0.6750\n",
      "Epoch 48/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6551 - accuracy: 0.6750\n",
      "Epoch 49/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6520 - accuracy: 0.6833\n",
      "Epoch 50/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6487 - accuracy: 0.6750\n",
      "Epoch 51/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6437 - accuracy: 0.6750\n",
      "Epoch 52/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6398 - accuracy: 0.6917\n",
      "Epoch 53/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6357 - accuracy: 0.6917\n",
      "Epoch 54/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6323 - accuracy: 0.6917\n",
      "Epoch 55/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6283 - accuracy: 0.7083\n",
      "Epoch 56/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6239 - accuracy: 0.7083\n",
      "Epoch 57/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6201 - accuracy: 0.7083\n",
      "Epoch 58/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6158 - accuracy: 0.6917\n",
      "Epoch 59/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6118 - accuracy: 0.6917\n",
      "Epoch 60/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6070 - accuracy: 0.7167\n",
      "Epoch 61/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6039 - accuracy: 0.7250\n",
      "Epoch 62/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5999 - accuracy: 0.7583\n",
      "Epoch 63/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5940 - accuracy: 0.7500\n",
      "Epoch 64/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5899 - accuracy: 0.7333\n",
      "Epoch 65/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5854 - accuracy: 0.7333\n",
      "Epoch 66/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5810 - accuracy: 0.7917\n",
      "Epoch 67/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5791 - accuracy: 0.7583\n",
      "Epoch 68/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5720 - accuracy: 0.8000\n",
      "Epoch 69/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5663 - accuracy: 0.8417\n",
      "Epoch 70/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5636 - accuracy: 0.7917\n",
      "Epoch 71/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5562 - accuracy: 0.8333\n",
      "Epoch 72/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5530 - accuracy: 0.9000\n",
      "Epoch 73/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5461 - accuracy: 0.9000\n",
      "Epoch 74/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5434 - accuracy: 0.8250\n",
      "Epoch 75/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5372 - accuracy: 0.8833\n",
      "Epoch 76/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5320 - accuracy: 0.9000\n",
      "Epoch 77/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5274 - accuracy: 0.9000\n",
      "Epoch 78/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5250 - accuracy: 0.8667\n",
      "Epoch 79/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5190 - accuracy: 0.9083\n",
      "Epoch 80/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5142 - accuracy: 0.9000\n",
      "Epoch 81/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5108 - accuracy: 0.9167\n",
      "Epoch 82/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5052 - accuracy: 0.8917\n",
      "Epoch 83/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5019 - accuracy: 0.9083\n",
      "Epoch 84/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4980 - accuracy: 0.9000\n",
      "Epoch 85/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4931 - accuracy: 0.9167\n",
      "Epoch 86/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4888 - accuracy: 0.9167\n",
      "Epoch 87/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4839 - accuracy: 0.9000\n",
      "Epoch 88/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4801 - accuracy: 0.9000\n",
      "Epoch 89/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4756 - accuracy: 0.9167\n",
      "Epoch 90/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4715 - accuracy: 0.9167\n",
      "Epoch 91/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4677 - accuracy: 0.9250\n",
      "Epoch 92/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4637 - accuracy: 0.9167\n",
      "Epoch 93/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4593 - accuracy: 0.9167\n",
      "Epoch 94/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4552 - accuracy: 0.9333\n",
      "Epoch 95/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4511 - accuracy: 0.9250\n",
      "Epoch 96/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4479 - accuracy: 0.9250\n",
      "Epoch 97/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4427 - accuracy: 0.9250\n",
      "Epoch 98/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4402 - accuracy: 0.9333\n",
      "Epoch 99/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4411 - accuracy: 0.9000\n",
      "Epoch 100/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4314 - accuracy: 0.9417\n",
      "Epoch 101/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4270 - accuracy: 0.9500\n",
      "Epoch 102/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4225 - accuracy: 0.9500\n",
      "Epoch 103/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4200 - accuracy: 0.9417\n",
      "Epoch 104/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4156 - accuracy: 0.9333\n",
      "Epoch 105/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4114 - accuracy: 0.9500\n",
      "Epoch 106/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4071 - accuracy: 0.9500\n",
      "Epoch 107/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4041 - accuracy: 0.9583\n",
      "Epoch 108/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3998 - accuracy: 0.9500\n",
      "Epoch 109/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3958 - accuracy: 0.9583\n",
      "Epoch 110/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3924 - accuracy: 0.9500\n",
      "Epoch 111/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3884 - accuracy: 0.9583\n",
      "Epoch 112/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3849 - accuracy: 0.9500\n",
      "Epoch 113/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3802 - accuracy: 0.9667\n",
      "Epoch 114/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3764 - accuracy: 0.9667\n",
      "Epoch 115/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3730 - accuracy: 0.9583\n",
      "Epoch 116/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3688 - accuracy: 0.9667\n",
      "Epoch 117/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3653 - accuracy: 0.9667\n",
      "Epoch 118/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3629 - accuracy: 0.9583\n",
      "Epoch 119/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3580 - accuracy: 0.9667\n",
      "Epoch 120/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3590 - accuracy: 0.9667\n",
      "Epoch 121/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3530 - accuracy: 0.9500\n",
      "Epoch 122/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3504 - accuracy: 0.9583\n",
      "Epoch 123/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3457 - accuracy: 0.9667\n",
      "Epoch 124/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3400 - accuracy: 0.9667\n",
      "Epoch 125/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3412 - accuracy: 0.9583\n",
      "Epoch 126/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3406 - accuracy: 0.9667\n",
      "Epoch 127/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3297 - accuracy: 0.9667\n",
      "Epoch 128/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3291 - accuracy: 0.9667\n",
      "Epoch 129/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3245 - accuracy: 0.9667\n",
      "Epoch 130/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3210 - accuracy: 0.9667\n",
      "Epoch 131/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3167 - accuracy: 0.9667\n",
      "Epoch 132/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3157 - accuracy: 0.9667\n",
      "Epoch 133/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3099 - accuracy: 0.9667\n",
      "Epoch 134/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3082 - accuracy: 0.9667\n",
      "Epoch 135/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3052 - accuracy: 0.9667\n",
      "Epoch 136/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3009 - accuracy: 0.9667\n",
      "Epoch 137/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2994 - accuracy: 0.9667\n",
      "Epoch 138/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2957 - accuracy: 0.9583\n",
      "Epoch 139/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2916 - accuracy: 0.9667\n",
      "Epoch 140/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2891 - accuracy: 0.9583\n",
      "Epoch 141/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2869 - accuracy: 0.9667\n",
      "Epoch 142/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2840 - accuracy: 0.9583\n",
      "Epoch 143/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2797 - accuracy: 0.9667\n",
      "Epoch 144/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2785 - accuracy: 0.9667\n",
      "Epoch 145/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2764 - accuracy: 0.9667\n",
      "Epoch 146/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2782 - accuracy: 0.9583\n",
      "Epoch 147/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2671 - accuracy: 0.9667\n",
      "Epoch 148/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2675 - accuracy: 0.9583\n",
      "Epoch 149/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2661 - accuracy: 0.9583\n",
      "Epoch 150/150\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2624 - accuracy: 0.9583\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1bb5e822950>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a neural network model\n",
    "# epochs=150: 150 times the model will be trained on the entire training dataset\n",
    "#   Too few epochs may result in underfitting, where the model hasn't learned the patterns in the data\n",
    "#   Too many epochs can lead to overfitting\n",
    "# batch_size=10: During each iteration through the dataset (epoch), the data is divided into batches, \n",
    "#   and the model's parameters are updated based on the error computed on each batch\n",
    "#     Smaller batch sizes may provide a regularizing effect and reduce overfitting\n",
    "#     Larger batch sizes can lead to faster convergence but may require more memory\n",
    "dnn.fit(X_train, y_train_one_hot, epochs=150, batch_size=10)\n",
    "\n",
    "# Small Datasets\n",
    "#   For small datasets, you might need more epochs and can use a smaller batch size. \n",
    "#   Common choices might be 10 to 100 epochs with a batch size of 8 to 32\n",
    "# Medium to Large Datasets\n",
    "#   For medium to large datasets, you may achieve good results with fewer epochs and larger batch sizes. \n",
    "#   Common choices might be 5 to 30 epochs with a batch size of 32 to 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec71a20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 317ms/step - loss: 0.2713 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.27130019664764404, 1.0]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the performance of a trained neural network model\n",
    "dnn.evaluate(X_test, y_test_one_hot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
